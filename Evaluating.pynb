{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Evaluating.pynb","provenance":[],"authorship_tag":"ABX9TyN7GZGryVr1p+hyLcXVxBZ1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eDsNLI5JcdCw","executionInfo":{"status":"ok","timestamp":1627143999300,"user_tz":-420,"elapsed":17268,"user":{"displayName":"Truong Hoang Gia Bao B1609809","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghtou-65Dwfppnq4KUy6WaS_PsZVMY9kT-QUuJC3g=s64","userId":"16980589557729516553"}},"outputId":"4bfab090-c0a1-41e5-df06-7e985917d3c1"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xkTZy7Q3d2h5","executionInfo":{"status":"ok","timestamp":1627144008534,"user_tz":-420,"elapsed":6911,"user":{"displayName":"Truong Hoang Gia Bao B1609809","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghtou-65Dwfppnq4KUy6WaS_PsZVMY9kT-QUuJC3g=s64","userId":"16980589557729516553"}}},"source":["from collections import Counter\n","from keras.models import load_model\n","from nltk.tokenize import word_tokenize\n","from nltk import ngrams,word_tokenize\n","import numpy as np\n","import re\n","import string\n","\n","model = load_model(\"gdrive/MyDrive/Spell Correction/256_spell.h5\")\n","\n","\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"3wqVOVPIeHkO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627144254974,"user_tz":-420,"elapsed":781,"user":{"displayName":"Truong Hoang Gia Bao B1609809","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghtou-65Dwfppnq4KUy6WaS_PsZVMY9kT-QUuJC3g=s64","userId":"16980589557729516553"}},"outputId":"5a644c12-6e70-4af8-85d6-6d6887a4ff32"},"source":["NGRAM=5\n","MAXLEN=40\n","alphabet = ['\\x00', ' ', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'á', 'à', 'ả', 'ã', 'ạ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ', 'ó', 'ò', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ', 'é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ', 'ú', 'ù', 'ủ', 'ũ', 'ụ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự', 'í', 'ì', 'ỉ', 'ĩ', 'ị', 'ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ', 'đ', 'Á', 'À', 'Ả', 'Ã', 'Ạ', 'Â', 'Ấ', 'Ầ', 'Ẩ', 'Ẫ', 'Ậ', 'Ă', 'Ắ', 'Ằ', 'Ẳ', 'Ẵ', 'Ặ', 'Ó', 'Ò', 'Ỏ', 'Õ', 'Ọ', 'Ô', 'Ố', 'Ồ', 'Ổ', 'Ỗ', 'Ộ', 'Ơ', 'Ớ', 'Ờ', 'Ở', 'Ỡ', 'Ợ', 'É', 'È', 'Ẻ', 'Ẽ', 'Ẹ', 'Ê', 'Ế', 'Ề', 'Ể', 'Ễ', 'Ệ', 'Ú', 'Ù', 'Ủ', 'Ũ', 'Ụ', 'Ư', 'Ứ', 'Ừ', 'Ử', 'Ữ', 'Ự', 'Í', 'Ì', 'Ỉ', 'Ĩ', 'Ị', 'Ý', 'Ỳ', 'Ỷ', 'Ỹ', 'Ỵ', 'Đ']\n","letters=list(\"abcdefghijklmnopqrstuvwxyzáàảãạâấầẩẫậăắằẳẵặóòỏõọôốồổỗộơớờởỡợéèẻẽẹêếềểễệúùủũụưứừửữựíìỉĩịýỳỷỹỵđABCDEFGHIJKLMNOPQRSTUVWXYZÁÀẢÃẠÂẤẦẨẪẬĂẮẰẲẴẶÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÉÈẺẼẸÊẾỀỂỄỆÚÙỦŨỤƯỨỪỬỮỰÍÌỈĨỊÝỲỶỸỴĐ\")\n","accepted_char=list((string.digits + ''.join(letters)))\n","\n","\n","def encoder_data(text, maxlen=MAXLEN):\n","        text = \"\\x00\" + text\n","        x = np.zeros((maxlen, len(alphabet)))\n","        for i, c in enumerate(text[:maxlen]):\n","            x[i, alphabet.index(c)] = 1\n","        if i < maxlen - 1:\n","          for j in range(i+1, maxlen):\n","            x[j, 0] = 1\n","        return x\n","      \n","def decoder_data(x):\n","    x = x.argmax(axis=-1)\n","    return ''.join(alphabet[i] for i in x)\n","\n","\n","def nltk_ngrams(words, n=5):\n","    return ngrams(words.split(), n)\n","      \n","def guess(ngram):\n","    text = ' '.join(ngram)\n","    preds = model.predict(np.array([encoder_data(text)]), verbose=0)\n","    return decoder_data(preds[0]).strip('\\x00')\n","\n","\n","\n","def correct(sentence):\n","    noise = []\n","    for i in sentence:\n","        if i not in accepted_char:\n","            sentence=sentence.replace(i,\" \")\n","    ngrams = list(nltk_ngrams(sentence, n=NGRAM))\n","    guessed_ngrams = list(guess(ngram) for ngram in ngrams)\n","    candidates = [Counter() for _ in range(len(guessed_ngrams)+ NGRAM - 1)]\n","    for nid, ngram in (enumerate(guessed_ngrams)):\n","        for wid, word in (enumerate(re.split(' +', ngram))):\n","            candidates[nid + wid].update([word])\n","    output = ' '.join(c.most_common(1)[0][0] for c in candidates)\n","\n","    print(sentence)\n","    for i in sentence.split():\n","        if i not in output.split():\n","            noise.append(i)\n","    return noise\n","\n","sentences = 'trong thuwj te61, modl cầnn phải cải thiệ thêm nhiều hon'\n","correct(sentences)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["trong thuwj te61  modl cầnn phải cải thiệ thêm nhiều hon\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['thuwj', 'te61', 'modl', 'thiệ', 'hon']"]},"metadata":{"tags":[]},"execution_count":5}]}]}